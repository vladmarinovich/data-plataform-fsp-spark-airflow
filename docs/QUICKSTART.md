# âš¡ Inicio RÃ¡pido - ExtracciÃ³n Real desde Supabase

## ğŸ¯ Resumen

Esta guÃ­a te lleva paso a paso desde cero hasta tener datos **reales** de Supabase procesados con PySpark.

---

## ğŸ“‹ Paso 1: Setup Inicial

```bash
# Navegar al proyecto
cd pyspark-airflow-data-platform

# Ejecutar setup automatizado
./scripts/setup.sh
```

**El script te preguntarÃ¡**:
```
Â¿Quieres usar datos MOCK para testing? (s/n)
```

- **Responde `n`** si quieres usar datos reales de Supabase
- **Responde `s`** si solo quieres probar con datos mock

---

## ğŸ”‘ Paso 2: Configurar Credenciales de Supabase

### 2.1 Obtener credenciales

1. Ve a [Supabase Dashboard](https://app.supabase.com)
2. Selecciona tu proyecto
3. Ve a **Settings â†’ API**
4. Copia:
   - **Project URL**: `https://xxxxxxxxxxx.supabase.co`
   - **anon/public key**: `eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...`

### 2.2 Editar archivo `.env`

```bash
nano .env
```

Reemplaza los valores:
```bash
SUPABASE_URL=https://xxxxxxxxxxx.supabase.co
SUPABASE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
```

Guarda y cierra (`Ctrl+O`, `Enter`, `Ctrl+X`)

---

## ğŸ“¥ Paso 3: Extraer Datos de Supabase

```bash
# Activar entorno virtual
source venv/bin/activate

# Ejecutar extracciÃ³n
python scripts/extract_from_supabase.py
```

**Salida esperada**:
```
============================================================
ğŸ Iniciando ExtracciÃ³n desde Supabase
============================================================
âœ… Conectado a Supabase: https://xxxxx.supabase.co

============================================================
ğŸš€ EXTRACCIÃ“N INCREMENTAL
============================================================

ğŸ“¥ Extrayendo tabla 'donaciones' (incremental)...
   âœ… ExtraÃ­dos 1543 registros nuevos
   
ğŸ“¥ Extrayendo tabla 'gastos' (incremental)...
   âœ… ExtraÃ­dos 892 registros nuevos

============================================================
ğŸ“¸ EXTRACCIÃ“N SNAPSHOT
============================================================

ğŸ“¸ Extrayendo tabla 'donantes' (snapshot completo)...
   âœ… ExtraÃ­dos 234 registros totales

============================================================
âœ… ExtracciÃ³n completada exitosamente
============================================================
```

### Verificar archivos generados:

```bash
ls -lh data/raw/

# DeberÃ­a mostrar:
# donaciones.parquet
# gastos.parquet
# donantes.parquet
# casos.parquet
# proveedores.parquet
```

---

## ğŸš€ Paso 4: Ejecutar TransformaciÃ³n PySpark

```bash
spark-submit --master local[*] jobs/transform_donations.py
```

**Salida esperada**:
```
============================================================
ğŸš€ Iniciando Job: Transform Donations
============================================================

ğŸ“¥ Leyendo datos Bronze (raw)...
ğŸ“Š DataFrame 'Bronze - Donaciones':
   - Filas: 1,543
   - Columnas: 7

ğŸ”„ Transformando a capa Silver (cleaned)...
âœ… Tabla 'donaciones': Todas las columnas requeridas presentes
âœ… Columna 'fecha_donacion': Todas las fechas vÃ¡lidas
âœ… Columna 'monto': Todos los montos positivos

ğŸ’¾ Escribiendo datos Silver...
âœ… Datos Silver escritos en: data/processed/silver/donaciones

ğŸ“Š Transformando a capa Gold (aggregated)...
ğŸ’¾ Escribiendo datos Gold...
âœ… Datos Gold escritos en: data/output/gold/donaciones_monthly

============================================================
âœ… Job completado exitosamente
============================================================
```

---

## âœ… Paso 5: Verificar Resultados

### Ver datos Silver (particionados):

```bash
ls -R data/processed/silver/donaciones/

# DeberÃ­a mostrar estructura:
# data/processed/silver/donaciones/:
# _SUCCESS  anio=2023  anio=2024
#
# data/processed/silver/donaciones/anio=2023:
# mes=01  mes=02  mes=03  ...
```

### Ver datos Gold (agregados):

```bash
ls -lh data/output/gold/donaciones_monthly/

# DeberÃ­a mostrar archivos Parquet
```

### Inspeccionar con PySpark:

```bash
pyspark
```

```python
# Leer datos Silver
df_silver = spark.read.parquet("data/processed/silver/donaciones")
df_silver.printSchema()
df_silver.show(5)

# Ver totales por mes
df_silver.groupBy("anio", "mes").count().orderBy("anio", "mes").show()

# Leer datos Gold
df_gold = spark.read.parquet("data/output/gold/donaciones_monthly")
df_gold.show()
```

---

## ğŸ”„ Ejecuciones Posteriores

### Extraer solo datos nuevos:

```bash
# Activar venv
source venv/bin/activate

# Extraer (solo trae registros nuevos)
python scripts/extract_from_supabase.py

# Transformar
spark-submit --master local[*] jobs/transform_donations.py
```

**Gracias a los watermarks**, solo se extraerÃ¡n registros con fechas posteriores a la Ãºltima extracciÃ³n.

---

## ğŸ› Troubleshooting

### Error: "Faltan credenciales de Supabase"

```bash
# Verificar .env
cat .env | grep SUPABASE

# DeberÃ­a mostrar tus credenciales
```

### Error: "Connection refused"

- Verifica que la URL sea correcta
- Verifica que el API Key sea el correcto
- Verifica que el proyecto no estÃ© pausado en Supabase

### No se extraen datos

```bash
# Ver watermarks actuales
cat watermarks.json

# Si estÃ¡n adelantados, resetear:
rm watermarks.json
```

### Error en transformaciÃ³n PySpark

```bash
# Verificar que existan los archivos de entrada
ls -lh data/raw/donaciones.parquet

# Si no existe, ejecutar extracciÃ³n primero
python scripts/extract_from_supabase.py
```

---

## ğŸ“š DocumentaciÃ³n Adicional

- **ExtracciÃ³n detallada**: [`docs/EXTRACCION_SUPABASE.md`](EXTRACCION_SUPABASE.md)
- **Arquitectura**: [`docs/ARCHITECTURE.md`](ARCHITECTURE.md)
- **README principal**: [`README.md`](../README.md)

---

## ğŸ¯ PrÃ³ximos Pasos

1. âœ… Configurar Airflow para automatizar
2. âœ… Agregar mÃ¡s transformaciones (gastos, casos, etc.)
3. âœ… Configurar deployment a cloud (AWS/GCP/Azure)
4. âœ… Agregar monitoreo y alertas

---

**Â¡Listo! Ahora tienes un pipeline funcional con datos reales de Supabase** ğŸš€
