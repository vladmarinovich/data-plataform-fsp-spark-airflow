
#  Adaptaciones T茅cnicas: De SQLX (BigQuery) a PySpark

Este documento detalla las decisiones de ingenier铆a tomadas al adaptar la l贸gica de Dataform (SQLX) a PySpark.

## 1. Manejo de Timestamps (Microsegundos vs Segundos)
### Adaptaci贸n
- **SQLX**: Usamos `TIMESTAMP_MICROS(DIV(col, 1000))`.
- **PySpark**: Usamos `F.from_unixtime(F.col(col)/1000000).cast("timestamp")`.

### Por qu茅 y Beneficio
Supabase/Postgres almacena marcas de tiempo como enteros en microsegundos. Spark, por defecto, interpreta los casts de enteros a timestamp como **segundos**. 
- **Beneficio**: Evitamos fechas err贸neas (a帽o 50,000+) y garantizamos que los datos en BigQuery (v铆a Parquet) mantengan la precisi贸n correcta.

### Trade-off
- **Carga Computacional**: Realizar una divisi贸n por cada registro en billones de registros es costoso en Spark. Sin embargo, dado el volumen actual del CRM (< 1TB), priorizamos la **integridad de la fecha** sobre micro-optimizaciones de CPU.

---

## 2. Particionamiento F铆sico (Hive-style)
### Adaptaci贸n
- **SQLX**: BigQuery maneja particiones l贸gicas (`partition by date`).
- **PySpark**: Creamos columnas expl铆citas `anio`, `mes`, `dia` y usamos `.partitionBy()`.

### Por qu茅 y Beneficio
En un Data Lake orientado a archivos (GCS/Local), Spark necesita una estructura de carpetas f铆sica (`/anio=2024/mes=12/`) para realizar **Partition Pruning**.
- **Beneficio**: Las consultas futuras solo leer谩n las carpetas necesarias, reduciendo costos de lectura en GCS en un 90%+.

### Trade-off
- **Estructura de Datos**: Agregamos 3 columnas "t茅cnicas" que no existen en el modelo relacional original.
- **Complejidad de Escritura**: Requiere configurar `partitionOverwriteMode = dynamic` para evitar borrar meses enteros al re-procesar un solo d铆a.

---

## 3. Deduplicaci贸n (Window Functions vs Qualify)
### Adaptaci贸n
- **SQLX**: Usa `QUALIFY ROW_NUMBER() OVER (...) = 1`.
- **PySpark**: Usa `F.row_number().over(window).filter(F.col("row_num") == 1)`.

### Por qu茅 y Beneficio
Spark no posee la clausula `QUALIFY`. Debemos materializar el `row_number` y luego filtrarlo.
- **Beneficio**: Es la forma m谩s robusta de implementar **CDC (Change Data Capture)** determin铆stico, asegurando que siempre nos quedamos con el `last_modified_at` m谩s reciente.

---

## 4. Normalizaci贸n de Medios de Pago (Regex-ish)
### Adaptaci贸n
- **SQLX**: `LIKE '%tarjeta%'`.
- **PySpark**: `.contains("tarjeta")`.

### Por qu茅 y Beneficio
Priorizamos `.contains()` sobre `regexp_like` por simplicidad y legibilidad.
- **Beneficio**: Mantenemos la l贸gica de negocio "difusa" (fuzzy logic) del SQLX original pero con sintaxis nativa de Spark.
