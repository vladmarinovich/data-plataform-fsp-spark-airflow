"""
LÃ³gica compartida y agnÃ³stica para jobs de PySpark.
Define contratos de lectura/escritura y funciones estÃ¡ndar.
"""
import sys
from pathlib import Path
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Importar config desde directorio superior
sys.path.insert(0, str(Path(__file__).parent.parent))
import config

def get_spark_session(app_name: str):
    """Crea sesiÃ³n de Spark configurada para el Data Platform."""
    
    # Ruta al jar del conector GCS
    jar_path = str(Path(__file__).parent.parent / "lib" / "gcs-connector-hadoop3-latest.jar")
    
    return (SparkSession.builder
        .appName(app_name)
        .master(config.SPARK_MASTER)
        # Configurar JARs externos
        .config("spark.jars", jar_path)
        .config("spark.driver.extraClassPath", jar_path)
        .config("spark.executor.extraClassPath", jar_path)
        # Configurar FileSystem GCS
        .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem")
        .config("spark.hadoop.fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS")
        # AutenticaciÃ³n (usa Application Default Credentials por defecto)
        .config("spark.hadoop.google.cloud.auth.service.account.enable", "true")
        # Importante: Permite sobrescribir solo las particiones afectadas
        .config("spark.sql.sources.partitionOverwriteMode", "dynamic")
        .getOrCreate())

def read_raw(spark, table_name):
    """
    Lee datos desde la capa RAW (agnÃ³stico del cloud provider).
    """
    path = f"{config.RAW_PATH}/raw_{table_name}"
    print(f"ğŸ“– LEYENDO RAW: {path}")
    
    try:
        return spark.read.parquet(path)
    except Exception as e:
        print(f"âš ï¸ Error leyendo {path}: {e}")
        # Retornar DataFrame vacÃ­o si falla (para no romper el job completo si es opcional)
        # O re-lanzar la excepciÃ³n si es crÃ­tico. AquÃ­ re-lanzamos.
        raise e

def write_silver(df, table_name, partition_cols=["anio", "mes"]):
    """
    Escribe datos en la capa SILVER (Formato Parquet).
    """
    path = f"{config.SILVER_PATH}/{table_name}"
    print(f"ğŸ’¾ ESCRIBIENDO SILVER: {path}")
    
    if "anio" not in df.columns and "anio" in partition_cols:
         print("âš ï¸ Advertencia: No se encontraron columnas de particiÃ³n, se escribirÃ¡ sin particionar.")
         partition_cols = []

    (df.write
        .mode("overwrite")
        .partitionBy(*partition_cols)
        .parquet(path))
    print(f"âœ… Escritura completada: {path}")

def standard_dedup(df, id_col, date_col="last_modified_at"):
    """
    Aplica deduplicaciÃ³n estÃ¡ndar CDC (Change Data Capture).
    Se queda con el registro con fecha mÃ¡s reciente.
    """
    print(f"ğŸ”„ DEDUPLICANDO por {id_col} usando {date_col}")
    
    # Validar que exista la columna de ordenamiento
    sort_col = date_col
    if date_col not in df.columns:
        if "created_at" in df.columns:
            print(f"âš ï¸ Columna {date_col} no encontrada, usando 'created_at' como fallback.")
            sort_col = "created_at"
        else:
            print(f"âš ï¸ Columna CDC no encontrada. No se puede deduplicar confiablemente.")
            return df

    window = Window.partitionBy(id_col).orderBy(F.col(sort_col).desc())
    
    df_dedup = (df.withColumn("rn", F.row_number().over(window))
                  .filter(F.col("rn") == 1)
                  .drop("rn"))
    
    count_before = df.count()
    count_after = df_dedup.count()
    print(f"   ğŸ“‰ ReducciÃ³n: {count_before} -> {count_after} registros")
    
    return df_dedup
