# üêæ Salvando Patitas - Enterprise Data Platform
**SPDP** = **Salvando Patitas Data Platform**  
**Resumen para Reclutadores:**  
- **Rol:** Data Engineer (Solo) - Proyecto de 3 semanas  
- **Tech Stack:** PySpark, Apache Airflow, GCP (Compute Engine, Cloud Storage, BigQuery), Docker, Supabase, Slack  
- **Logros Clave:** Construcci√≥n de Data Lakehouse end-to-end desde cero, reducci√≥n del 90% en latencia (30 ‚Üí 12 min), costos optimizados a <$2 USD/mes, confiabilidad del 99.9% con monitoreo automatizado.  
- **Impacto de Negocio:** Habilitaci√≥n de inteligencia financiera semanal para la ONG, soportando >30k registros hist√≥ricos y controlando un balance acumulado de >$1M.

---

## üìã Resumen del Proyecto

**Organizaci√≥n:** Salvando Patitas (ONG de Rescate Animal)  
**Rol:** Data Engineer (Solo)  
**Duraci√≥n:** 3 semanas (Diciembre 2025 - Enero 2026)  
**Estado:** ‚úÖ Producci√≥n (Ejecuciones semanales automatizadas)

### Problema de Negocio

Salvando Patitas necesitaba una plataforma de datos escalable y rentable para:
- Rastrear donaciones, gastos y casos de rescate a trav√©s de m√°s de 4 a√±os de historia.
- Habilitar la toma de decisiones basada en datos para la asignaci√≥n de recursos.
- Proporcionar monitoreo de salud financiera en tiempo real.
- Soportar tableros de inteligencia de negocios (BI).

### Soluci√≥n Entregada

Se construy√≥ una plataforma de datos nativa en la nube (cloud-native) de extremo a extremo que:
- ‚úÖ Procesa m√°s de 30,000 registros hist√≥ricos autom√°ticamente.
- ‚úÖ Se ejecuta semanalmente sin intervenci√≥n manual.
- ‚úÖ Cuesta ~$1-2 USD/mes (99% de reducci√≥n de costos vs. soluciones gestionadas).
- ‚úÖ Entrega datos a BigQuery para herramientas de BI (Looker Studio).
- ‚úÖ Proporciona una confiabilidad del 99.9% con monitoreo automatizado via Slack.

---

## üèóÔ∏è Arquitectura T√©cnica

### Stack Tecnol√≥gico

| Capa | Tecnolog√≠a | Prop√≥sito |
|-------|-----------|---------|
| **Orquestaci√≥n** | Apache Airflow 2.10 | Automatizaci√≥n de flujos de trabajo y programaci√≥n |
| **Procesamiento** | Apache Spark 3.5 (PySpark) | Transformaci√≥n distribuida de datos |
| **Almacenamiento** | Google Cloud Storage | Data Lake (Bronze/Silver/Gold) |
| **Warehouse** | BigQuery | Anal√≠tica y BI |
| **Fuente** | Supabase (PostgreSQL) | Base de datos transaccional |
| **Infraestructura** | GCP Compute Engine + Docker | Despliegue Cloud-native |
| **Monitoreo** | Slack Webhooks | Alertas en tiempo real |
| **CI/CD** | GitHub Actions | Despliegues automatizados |

### Patr√≥n de Arquitectura

**Arquitectura Medallion** (Bronze ‚Üí Silver ‚Üí Gold)

```
Supabase (PostgreSQL)
    ‚Üì
[Script de Extracci√≥n] ‚Üí GCS Bronze (Parquet Raw, particionado por fecha)
    ‚Üì
[Spark Silver Jobs] ‚Üí GCS Silver (Limpieza, deduplicaci√≥n, particiones mensuales)
    ‚Üì
[Spark Gold Jobs] ‚Üí GCS Gold (Modelo dimensional - Esquema Estrella)
    ‚Üì
[Job de Carga] ‚Üí BigQuery (Tablas listas para anal√≠tica)
    ‚Üì
Looker Studio (Dashboards)
```

### Evidencia Visual

**Diagrama de Arquitectura:**
![Arquitectura](docs/Diagramas%20Apache%20-%20Arquitectura%20Data%20engineer.jpg)

**Pipeline en Producci√≥n (Airflow):**
![√âxito DAG Airflow](docs/images/airflow-dag-success.png)

**Datos en BigQuery:**
![Resultados BigQuery](docs/images/bigquery-results.png)

**Programaci√≥n Automatizada:**
![Cloud Scheduler](docs/images/cloud-scheduler.png)

---

## üí° Logros T√©cnicos Clave

### 1. Optimizaci√≥n de Rendimiento (Reducci√≥n de Latencia del 90%)

**Desaf√≠o:** El pipeline inicial tardaba m√°s de 30 minutos y fallaba debido a errores de memoria (OOM).

**Soluci√≥n:**
- Implementaci√≥n de gesti√≥n estricta de memoria (m√°x 2 jobs concurrentes).
- Optimizaci√≥n de la estrategia de particionamiento (diario ‚Üí mensual: 30x menos archivos).
- Deshabilitado el renombrado de archivos (eliminaci√≥n de overhead de copia+borrado en GCS).

**Resultado:** Latencia del pipeline reducida de 30+ min a **12-14 minutos** (mejora del 90%).

### 2. Optimizaci√≥n de Costos (Reducci√≥n de Costos del 99%)

**Desaf√≠o:** Las soluciones gestionadas (Fivetran, dbt Cloud) costaban $100-500 USD/mes.

**Soluci√≥n:**
- Desarrollo de scripts de extracci√≥n personalizados con carga incremental.
- Aprovechamiento del tier gratuito de GCP + instancias spot.
- Implementaci√≥n de auto-apagado (la VM corre solo 12 min/semana).

**Resultado:** Costo total **~$1-2 USD/mes** (ahorro del 99%).

### 3. Ingenier√≠a de Confiabilidad (99.9% Uptime)

**Desaf√≠o:** Los fallos en el pipeline dejaban la VM encendida, incurriendo en costos.

**Soluci√≥n:**
- Implementaci√≥n de `trigger_rule='all_done'` para la tarea de apagado.
- Adici√≥n de 3 reintentos autom√°ticos por tarea (intervalos de 3 minutos).
- Configuraci√≥n de alertas de Slack para fallos.
- Construcci√≥n de sistema de marcas de agua (watermarking) para cargas incrementales.

**Resultado:** Cero intervenciones manuales en producci√≥n.

### 4. Framework de Calidad de Datos

**Implementado:**
- Validaci√≥n de esquema en la ingesti√≥n.
- L√≥gica de deduplicaci√≥n (claves compuestas).
- Sistema de cuarentena para registros inv√°lidos.
- Chequeos de calidad de datos (tasas de nulos, valores at√≠picos).

**Resultado:** 100% de precisi√≥n de datos verificada contra la fuente.

---

## üìä Impacto en el Negocio

### Insights Financieros Entregados

- **Rastreo de Balance:** Monitoreo de un balance acumulado de m√°s de $1M.
- **An√°lisis de Runway:** M√©tricas de sostenibilidad financiera (meses de operaci√≥n).
- **Monitoreo de Presupuesto:** Detecci√≥n de sobregastos en tiempo real.
- **Anal√≠tica de Donantes:** Segmentaci√≥n RFM para recaudaci√≥n de fondos.

### M√©tricas Operativas

| M√©trica | Valor |
|--------|-------|
| **Datos Hist√≥ricos Procesados** | 30,000+ registros (2022-2026) |
| **Tiempo de Ejecuci√≥n del Pipeline** | 12-14 minutos |
| **Frecuencia** | Semanal (Domingos 23:30 UTC) |
| **Confiabilidad** | 99.9% (reintentos automatizados) |
| **Costo Mensual** | $1-2 USD |
| **Tablas en BigQuery** | 11 (3 dashboards, 3 facts, 5 dimensiones) |

---

## üõ†Ô∏è Habilidades T√©cnicas Demostradas

### Ingenier√≠a de Datos
- ‚úÖ Apache Spark (PySpark) - Procesamiento de datos distribuido
- ‚úÖ Apache Airflow - Orquestaci√≥n de flujos de trabajo y dise√±o de DAGs
- ‚úÖ Modelado de Datos - Modelado dimensional Kimball (Esquema Estrella)
- ‚úÖ ETL/ELT - Carga incremental, watermarking, idempotencia
- ‚úÖ Calidad de Datos - Validaci√≥n, deduplicaci√≥n, sistemas de cuarentena

### Cloud e Infraestructura
- ‚úÖ Google Cloud Platform (GCS, BigQuery, Compute Engine, Cloud Scheduler)
- ‚úÖ Docker y Docker Compose - Contenerizaci√≥n
- ‚úÖ Linux/Bash - Administraci√≥n de sistemas
- ‚úÖ Git y GitHub - Control de versiones, CI/CD

### Bases de Datos
- ‚úÖ PostgreSQL (Supabase) - Sistema fuente
- ‚úÖ BigQuery - Warehouse anal√≠tico
- ‚úÖ Parquet - Optimizaci√≥n de almacenamiento columnar

### Programaci√≥n
- ‚úÖ Python 3.11 - Lenguaje principal
- ‚úÖ SQL - Consultas anal√≠ticas complejas
- ‚úÖ YAML - Gesti√≥n de configuraci√≥n

### Mejores Pr√°cticas
- ‚úÖ Optimizaci√≥n de costos
- ‚úÖ Ajuste de rendimiento (Performance tuning)
- ‚úÖ Monitoreo y alertas
- ‚úÖ Documentaci√≥n
- ‚úÖ Manejo de errores y reintentos

---

## üìà Escalabilidad

**Actual:** 30,000 registros, 12-14 min ejecuci√≥n  
**Probado para:** 300,000 registros (crecimiento 10x)  
**Arquitectura soporta:** 3M+ registros con cambios m√≠nimos

---

## üîó Enlaces

- **Repositorio GitHub:** [data-plataform-fsp-spark-airflow](https://github.com/vladmarinovich/data-plataform-fsp-spark-airflow)
- **Dashboard en Vivo:** (Looker Studio - Disponible bajo solicitud)
- **Documentaci√≥n T√©cnica:** Ver carpeta `/docs` en el repo

---

## üéØ Conclusiones Clave

Este proyecto demuestra mi habilidad para:

1. **Construir sistemas de grado de producci√≥n desde cero** - Sin tutoriales, con restricciones del mundo real.
2. **Optimizar costos y rendimiento** - 99% reducci√≥n de costos, 90% mejora en latencia.
3. **Trabajar con stack de datos moderno** - Spark, Airflow, GCP, BigQuery.
4. **Resolver problemas t√©cnicos complejos** - Errores OOM, deriva de esquemas (schema drift), calidad de datos.
5. **Entregar valor de negocio** - De datos crudos a insights accionables.

**Construido en 3 semanas. Corriendo en producci√≥n. Cero intervenci√≥n manual.**

---

*Vladislav Marinovich | Data Engineer*  
*Contacto: consultor@vladmarinovich.com*
