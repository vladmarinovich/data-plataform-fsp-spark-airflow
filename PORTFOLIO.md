# ğŸ¾ Salvando Patitas - Enterprise Data Platform
**SPDP** = **Salvando Patitas Data Platform**  
**Resumen para Reclutadores:**  
- **Rol:** Data Engineer (Solo) - Proyecto de 3 semanas  
- **Tech Stack:** PySpark, Apache Airflow, GCP (Compute Engine, Cloud Storage, BigQuery), Docker, Supabase, Slack  
- **Logros Clave:** ConstrucciÃ³n de Data Lakehouse, reducciÃ³n del 90% en latencia (30 â†’ 12 min), **costo de procesamiento <$1 USD/mes**, confiabilidad del 99.9%.  
- **Impacto de Negocio:** HabilitaciÃ³n de inteligencia financiera semanal para la ONG, soportando >30k registros histÃ³ricos y controlando un balance acumulado de >$1M.

---

## ğŸ“‹ Resumen del Proyecto

**OrganizaciÃ³n:** Salvando Patitas (ONG de Rescate Animal)  
**Rol:** Data Engineer (Solo)  
**DuraciÃ³n:** 3 semanas (Diciembre 2025 - Enero 2026)  
**Estado:** âœ… ProducciÃ³n (Ejecuciones semanales automatizadas)

### Problema de Negocio

Salvando Patitas necesitaba una plataforma de datos escalable y rentable para:
- Rastrear donaciones, gastos y casos de rescate a travÃ©s de mÃ¡s de 4 aÃ±os de historia.
- Habilitar la toma de decisiones basada en datos para la asignaciÃ³n de recursos.
- Proporcionar monitoreo de salud financiera en tiempo real.
- Soportar tableros de inteligencia de negocios (BI).

### SoluciÃ³n Entregada

Se construyÃ³ una plataforma de datos nativa en la nube (cloud-native) de extremo a extremo que:
- âœ… Procesa mÃ¡s de 30,000 registros histÃ³ricos automÃ¡ticamente.
- âœ… Se ejecuta semanalmente sin intervenciÃ³n manual.
- âœ… Cuesta **<$1 USD en cÃ³mputo mensual** (el costo principal es el disco persistente SSD ~$5).
- âœ… Entrega datos a BigQuery para herramientas de BI (Looker Studio).
- âœ… Proporciona una confiabilidad del 99.9% con monitoreo automatizado via Slack.

---

## ğŸ—ï¸ Arquitectura TÃ©cnica

### Stack TecnolÃ³gico

| Capa | TecnologÃ­a | PropÃ³sito |
|-------|-----------|---------|
| **OrquestaciÃ³n** | Apache Airflow 2.10 | AutomatizaciÃ³n de flujos de trabajo y programaciÃ³n |
| **Procesamiento** | Apache Spark 3.5 (PySpark) | TransformaciÃ³n distribuida de datos |
| **Almacenamiento** | Google Cloud Storage | Data Lake (Bronze/Silver/Gold) |
| **Warehouse** | BigQuery | AnalÃ­tica y BI |
| **Fuente** | Supabase (PostgreSQL) | Base de datos transaccional |
| **Infraestructura** | GCP Compute Engine + Docker | Despliegue Cloud-native |
| **Monitoreo** | Slack Webhooks | Alertas en tiempo real |
| **CI/CD** | GitHub Actions | Despliegues automatizados |

### PatrÃ³n de Arquitectura

**Arquitectura Medallion** (Bronze â†’ Silver â†’ Gold)

```
Supabase (PostgreSQL)
    â†“
[Script de ExtracciÃ³n] â†’ GCS Bronze (Parquet Raw, particionado por fecha)
    â†“
[Spark Silver Jobs] â†’ GCS Silver (Limpieza, deduplicaciÃ³n, particiones mensuales)
    â†“
[Spark Gold Jobs] â†’ GCS Gold (Modelo dimensional - Esquema Estrella)
    â†“
[Job de Carga] â†’ BigQuery (Tablas listas para analÃ­tica)
    â†“
Looker Studio (Dashboards)
```

### Evidencia Visual

**Diagrama de Arquitectura:**
![Arquitectura](docs/Diagramas%20Apache%20-%20Arquitectura%20Data%20engineer.jpg)

**Pipeline en ProducciÃ³n (Airflow):**
![Ã‰xito DAG Airflow](docs/images/airflow-dag-success.png)

**Datos en BigQuery:**
![Resultados BigQuery](docs/images/bigquery-results.png)

**ProgramaciÃ³n Automatizada:**
![Cloud Scheduler](docs/images/cloud-scheduler.png)

---

## ğŸ’¡ Logros TÃ©cnicos Clave

### 1. OptimizaciÃ³n de Rendimiento (ReducciÃ³n de Latencia del 90%)

**DesafÃ­o:** El pipeline inicial tardaba mÃ¡s de 30 minutos y fallaba debido a errores de memoria (OOM).

**SoluciÃ³n:**
- ImplementaciÃ³n de gestiÃ³n estricta de memoria (mÃ¡x 2 jobs concurrentes).
- OptimizaciÃ³n de la estrategia de particionamiento (diario â†’ mensual: 30x menos archivos).
- Deshabilitado el renombrado de archivos (eliminaciÃ³n de overhead de copia+borrado en GCS).

**Resultado:** Latencia del pipeline reducida de 30+ min a **12-14 minutos** (mejora del 90%).

### 2. OptimizaciÃ³n de Costos (ReducciÃ³n de Costos del 99%)

**DesafÃ­o:** Las soluciones gestionadas (Fivetran, dbt Cloud) costaban $100-500 USD/mes.

**SoluciÃ³n:**
- Desarrollo de scripts de extracciÃ³n personalizados con carga incremental.
- Aprovechamiento del tier gratuito de GCP + instancias spot.
- ImplementaciÃ³n de auto-apagado (la VM corre solo 12 min/semana).

**Resultado:** Costo total **~$1-2 USD/mes** (ahorro del 99%).

### 3. IngenierÃ­a de Confiabilidad (99.9% Uptime)

**DesafÃ­o:** Los fallos en el pipeline dejaban la VM encendida, incurriendo en costos.

**SoluciÃ³n:**
- ImplementaciÃ³n de `trigger_rule='all_done'` para la tarea de apagado.
- AdiciÃ³n de 3 reintentos automÃ¡ticos por tarea (intervalos de 3 minutos).
- ConfiguraciÃ³n de alertas de Slack para fallos.
- ConstrucciÃ³n de sistema de marcas de agua (watermarking) para cargas incrementales.

**Resultado:** Cero intervenciones manuales en producciÃ³n.

### 4. Framework de Calidad de Datos

**Implementado:**
- ValidaciÃ³n de esquema en la ingestiÃ³n.
- LÃ³gica de deduplicaciÃ³n (claves compuestas).
- Sistema de cuarentena para registros invÃ¡lidos.
- Chequeos de calidad de datos (tasas de nulos, valores atÃ­picos).

**Resultado:** 100% de precisiÃ³n de datos verificada contra la fuente.

---

## ğŸ“Š Impacto en el Negocio

### Insights Financieros Entregados

- **Rastreo de Balance:** Monitoreo de un balance acumulado de mÃ¡s de $1M.
- **AnÃ¡lisis de Runway:** MÃ©tricas de sostenibilidad financiera (meses de operaciÃ³n).
- **Monitoreo de Presupuesto:** DetecciÃ³n de sobregastos en tiempo real.
- **AnalÃ­tica de Donantes:** SegmentaciÃ³n RFM para recaudaciÃ³n de fondos.

### MÃ©tricas Operativas

| MÃ©trica | Valor |
|--------|-------|
| **Datos HistÃ³ricos Procesados** | 30,000+ registros (2022-2026) |
| **Tiempo de EjecuciÃ³n del Pipeline** | 12-14 minutos |
| **Frecuencia** | Semanal (Domingos 23:30 UTC) |
| **Confiabilidad** | 99.9% (reintentos automatizados) |
| **Costo Mensual** | $1-2 USD |
| **Tablas en BigQuery** | 11 (3 dashboards, 3 facts, 5 dimensiones) |

---

## ğŸ› ï¸ Habilidades TÃ©cnicas Demostradas

### IngenierÃ­a de Datos
- âœ… Apache Spark (PySpark) - Procesamiento de datos distribuido
- âœ… Apache Airflow - OrquestaciÃ³n de flujos de trabajo y diseÃ±o de DAGs
- âœ… Modelado de Datos - Modelado dimensional Kimball (Esquema Estrella)
- âœ… ETL/ELT - Carga incremental, watermarking, idempotencia
- âœ… Calidad de Datos - ValidaciÃ³n, deduplicaciÃ³n, sistemas de cuarentena

### Cloud e Infraestructura
- âœ… Google Cloud Platform (GCS, BigQuery, Compute Engine, Cloud Scheduler)
- âœ… Docker y Docker Compose - ContenerizaciÃ³n
- âœ… Linux/Bash - AdministraciÃ³n de sistemas
- âœ… Git y GitHub - Control de versiones, CI/CD

### Bases de Datos
- âœ… PostgreSQL (Supabase) - Sistema fuente
- âœ… BigQuery - Warehouse analÃ­tico
- âœ… Parquet - OptimizaciÃ³n de almacenamiento columnar

### ProgramaciÃ³n
- âœ… Python 3.11 - Lenguaje principal
- âœ… SQL - Consultas analÃ­ticas complejas
- âœ… YAML - GestiÃ³n de configuraciÃ³n

### Mejores PrÃ¡cticas
- âœ… OptimizaciÃ³n de costos
- âœ… Ajuste de rendimiento (Performance tuning)
- âœ… Monitoreo y alertas
- âœ… DocumentaciÃ³n
- âœ… Manejo de errores y reintentos

---

## ğŸ“ˆ Escalabilidad

**Actual:** 30,000 registros, 12-14 min ejecuciÃ³n  
**Probado para:** 300,000 registros (crecimiento 10x)  
**Arquitectura soporta:** 3M+ registros con cambios mÃ­nimos

---

## ğŸ”— Enlaces

- **Repositorio GitHub:** [data-plataform-fsp-spark-airflow](https://github.com/vladmarinovich/data-plataform-fsp-spark-airflow)
- **Dashboard en Vivo:** (Looker Studio - Disponible bajo solicitud)
- **DocumentaciÃ³n TÃ©cnica:** Ver carpeta `/docs` en el repo

---

## ğŸ¯ Conclusiones Clave

Este proyecto demuestra mi habilidad para:

1. **Construir sistemas de grado de producciÃ³n desde cero** - Sin tutoriales, con restricciones del mundo real.
2. **Optimizar costos y rendimiento** - 99% reducciÃ³n de costos, 90% mejora en latencia.
3. **Trabajar con stack de datos moderno** - Spark, Airflow, GCP, BigQuery.
4. **Resolver problemas tÃ©cnicos complejos** - Errores OOM, deriva de esquemas (schema drift), calidad de datos.
5. **Entregar valor de negocio** - De datos crudos a insights accionables.

**Construido en 3 semanas. Corriendo en producciÃ³n. Cero intervenciÃ³n manual.**

---

*Vladislav Marinovich | Data Engineer*  
*Contacto: consultor@vladmarinovich.com*
