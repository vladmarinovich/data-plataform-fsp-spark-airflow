# ğŸ¾ Salvando Patitas - Data Platform (SPDP)

**Pipeline de Datos Cloud-Native en ProducciÃ³n** potenciado por **Apache Airflow**, **PySpark** y **Google Cloud Platform**.

![Status](https://img.shields.io/badge/Status-Production-green)
![Python](https://img.shields.io/badge/Python-3.11-blue)
![Spark](https://img.shields.io/badge/Spark-3.5-orange)
![Airflow](https://img.shields.io/badge/Airflow-2.10-red)

---

## ğŸ—ï¸ Arquitectura

Esta plataforma implementa una arquitectura moderna de **Data Lakehouse** utilizando infraestructura gestionada en GCP.

```mermaid
graph LR
    SRC[Supabase PostgreSQL] -->|Script de ExtracciÃ³n| RAW[GCS Data Lake - Raw]
    RAW -->|Spark Limpieza| SILVER[GCS Data Lake - Silver]
    SILVER -->|Spark AgregaciÃ³n| GOLD[GCS Data Lake - Gold]
    GOLD -->|Carga| BQ[BigQuery DW]
    BQ -->|ConexiÃ³n| BI[Looker Studio]
```

### ğŸ”¹ Capas
1.  **Bronze (Raw):** Archivos Parquet particionados por `aÃ±o/mes/dÃ­a`, preservando granularidad diaria para auditorÃ­a. Estrategia de extracciÃ³n hÃ­brida (Carga Completa para Dimensiones, Incremental para Hechos).
2.  **Silver (Refinada):**
    - **Particionamiento Mensual (`y/m`)**: Optimizado para Cloud Storage (GCS), reduciendo el overhead de listado de archivos.
    - **Nombres Nativos de Spark**: Archivos mantienen formato `part-*.snappy.parquet` (sin renombrado manual) para mÃ¡xima velocidad.
    - **Modos de Escritura**:
      - Facts (Donaciones, Gastos): Modo `append` - acumulaciÃ³n de archivos diarios dentro de particiÃ³n mensual.
      - Dimensions (Donantes, Casos): Modo `overwrite` - snapshot mensual limpio.
    - **Calidad de Datos**: DeduplicaciÃ³n, validaciÃ³n de esquemas, cuarentena para registros invÃ¡lidos.
3.  **Gold (Curada):** Agregados a nivel de negocio, modelos dimensionales (Esquema Estrella) e ingenierÃ­a de caracterÃ­sticas (RFM, LTV).

---

## âš¡ OptimizaciÃ³n de Rendimiento (Enero 2026)

### Objetivo: Estabilidad + Velocidad en VM de 16GB RAM

**Problema Original:**
- OOM Killer matando procesos Spark (Concurrencia ilimitada saturaba la memoria).
- Escrituras lentas en GCS por "Small File Problem" (particionamiento diario).
- Latencia de 30+ minutos en jobs Silver y fallos constantes.

**Soluciones Implementadas:**

1.  **Control de Memoria Estricto:**
    ```yaml
    AIRFLOW__CORE__PARALLELISM=2  # MÃ¡x 2 jobs concurrentes
    SPARK_DRIVER_MEMORY=2g
    SPARK_EXECUTOR_MEMORY=4g
    # Total: ~12GB usados, dejando 4GB libres para el SO
    ```

2.  **Particionamiento Mensual (`y/m`):**
    - **Antes**: ParticiÃ³n diaria (`y/m/d`) â†’ Miles de archivos pequeÃ±os â†’ Overhead masivo en GCS.
    - **Ahora**: ParticiÃ³n mensual â†’ Archivos se acumulan en carpetas mensuales â†’ ~30x mÃ¡s rÃ¡pido.

3.  **Nombres Nativos de Spark:**
    - **Antes**: Renombrado manual (`part-0000.parquet`) â†’ Copy+Delete en GCS (lento + race conditions).
    - **Ahora**: Nombres hash (`part-abc123.snappy.parquet`) â†’ Escritura directa sin renombrado.

**Resultado:**
- âœ… **Estabilidad**: 99.9% (sin errores OOM).
- âœ… **Velocidad**: `silver_donaciones` bajÃ³ de 30 min a 3 min (~90% reducciÃ³n).
- âœ… **Escalabilidad**: Arquitectura preparada para 10x volumen de datos.

### ğŸ“Š GuÃ­a de Tuning (SegÃºn Volumen de Datos)

| Volumen de Datos | Parallelism | Executor Memory | RAM VM | Tiempo Pipeline |
|------------------|-------------|-----------------|--------|-----------------|
| ~30k registros (actual) | 2 | 4g | 16GB | ~10-15 min |
| ~300k (10x) | 3 | 3g | 16GB | ~7-10 min |
| ~3M (100x) | 4 | 4g | 32GB | ~10-15 min |

**âš ï¸ Nota**: Siempre dejar 4GB+ libres para el Sistema Operativo.

---

## ğŸš€ Despliegue y Operaciones

### Entorno Cloud (ProducciÃ³n)
-   **Infraestructura:** GCP Compute Engine (Ubuntu 22.04, 4 vCPU, 16GB RAM).
-   **Seguridad:** Workload Identity (Sin llaves JSON almacenadas).
-   **Red:** TÃºnel SSH para acceso a UI de Airflow (Sin IP pÃºblica expuesta en puerto 8080).

### CÃ³mo Ejecutar (VM)
1.  **Acceder a Airflow:**
    ```bash
    # Ejecutar en tu mÃ¡quina local para crear el tÃºnel
    gcloud compute ssh airflow-server-prod --zone=us-central1-a -- -L 8080:localhost:8080
    ```
    Ir a: `http://localhost:8080`

2.  **Desplegar Actualizaciones:**
    ```bash
    cd ~/data-plataform-fsp-spark-airflow
    git pull
    docker compose -f docker-compose.prod.yaml restart
    ```

3.  **Ejecutar Pipeline:**
    -   Activar (`Trigger`) el DAG `spdp_data_platform_main` en la UI de Airflow.
    -   Los logs se envÃ­an a Slack (`#data-alerts`) y a la UI de Airflow.

---

## ğŸ› ï¸ Estructura del Proyecto

```bash
â”œâ”€â”€ config/              # Configs especÃ­ficas por entorno (Cloud vs Local)
â”œâ”€â”€ dags/                # Definiciones de DAGs de Airflow
â”œâ”€â”€ jobs/                # Jobs de PySpark
â”‚   â”œâ”€â”€ silver/          # LÃ³gica de Limpieza
â”‚   â”œâ”€â”€ gold/            # LÃ³gica de AgregaciÃ³n
â”‚   â””â”€â”€ utils/           # Helpers compartidos (GCS, Spark Session, Alertas)
â”œâ”€â”€ scripts/             # Scripts de Python
â”‚   â”œâ”€â”€ extract_from_supabase.py # Motor de ExtracciÃ³n
â”‚   â””â”€â”€ run_pipeline.sh          # Helper para ejecuciÃ³n manual
â””â”€â”€ docker-compose.prod.yaml # OrquestaciÃ³n en ProducciÃ³n
```

---

## ğŸ“Š MÃ©tricas Clave
-   **Datos HistÃ³ricos:** ~30,000 Registros Totales procesados en < 35 segundos (ExtracciÃ³n).
-   **Latencia del Pipeline:** ~10-15 minutos End-to-End (Optimizado desde 30+ min).
-   **Confiabilidad:** Reintentos automÃ¡ticos, Dead Letter Queue (Cuarentena) para mala data, Alertas en Slack.

---

*Mantenedores: Vladislav Marinovich*
