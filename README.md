
# ğŸš€ PySpark + GCP Data Platform (Salvando Patitas)

> **Modern Data Platform** diseÃ±ada para la ingesta, transformaciÃ³n y anÃ¡lisis de datos de Salvando Patitas, utilizando **Supabase**, **Google Cloud Storage (GCS)**, **BigQuery** y **Apache Spark**.

---

## ğŸ“‹ Tabla de Contenidos

- [Arquitectura](#-arquitectura)
- [Workflow de Datos](#-workflow-de-datos)
- [Requisitos Previos](#-requisitos-previos)
- [InstalaciÃ³n](#-instalaciÃ³n)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [EjecuciÃ³n Capa RAW](#-ejecuciÃ³n-capa-raw)
- [Variables de Entorno](#-variables-de-entorno)

---

## ğŸ—ï¸ Arquitectura

Este proyecto implementa una arquitectura **Medallion (Bronze/Silver/Gold)** sobre Google Cloud Platform:

1.  **Fuente**: Supabase (PostgreSQL).
2.  **Raw (Bronze)**: Datos crudos en formato **Parquet** almacenados en **GCS**, particionados por `anio/mes/dia`. Expuestos como *External Tables* en **BigQuery**.
3.  **Silver (En proceso)**: Datos limpios, deduplicados y tipados (Spark Jobs).
4.  **Gold (En proceso)**: Agregaciones de negocio para dashboards.

---

## ğŸ”„ Workflow de Datos

El pipeline actual cubre la capa RAW completa:

1.  **ExtracciÃ³n**: `scripts/extract_from_supabase.py`
    *   Descarga incremental (usando `watermarks.json`) o Full Load.
    *   Guarda localmente en `data/raw/*.parquet`.
2.  **Carga a GCS**: `scripts/upload_to_gcs.py`
    *   Sube archivos a `gs://salvando-patitas-spark-raw/`.
    *   Aplica **Particionamiento Hive** (`anio=YYYY/mes=MM/dia=DD`) para tablas transaccionales.
    *   Convierte tipos crÃ­ticos (IDs a INT64, Fechas a UTC Microseconds).
3.  **DefiniciÃ³n**: `scripts/create_external_tables.py`
    *   Crea o actualiza tablas externas en BigQuery (`raw_donaciones`, `raw_casos`, etc.).
    *   Configura detecciÃ³n de particiones automÃ¡tica.

---

## âœ… Requisitos Previos

*   **Python 3.9+**
*   **Google Cloud SDK** (gcloud) autenticado.
*   **Cuenta de Servicio GCP** (o credenciales de usuario con permisos de Storage Admin y BigQuery Admin).

---

## ğŸ”§ InstalaciÃ³n

### 1. Clonar y Configurar entorno

```bash
git clone <repo-url>
cd pyspark-airflow-data-platform
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Configurar Variables de Entorno

Copiar `.env.example` a `.env` y configurar:

```ini
# GCP
GOOGLE_APPLICATION_CREDENTIALS="path/to/credentials.json" # Opcional si usas gcloud auth application-default

# Supabase
SUPABASE_URL="https://tu-proyecto.supabase.co"
SUPABASE_KEY="tu-service-role-key"

# Spark
SPARK_MASTER="local[*]"
```

---

## ğŸ“ Estructura del Proyecto

```
pyspark-airflow-data-platform/
â”‚
â”œâ”€â”€ config/                  # ConfiguraciÃ³n central (paths, schemas)
â”œâ”€â”€ jobs/                    # Spark Jobs (Silver/Gold)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract_from_supabase.py   # ETL Supabase -> Local
â”‚   â”œâ”€â”€ upload_to_gcs.py           # ETL Local -> GCS (Partitioned)
â”‚   â””â”€â”€ create_external_tables.py  # DDL BigQuery External
â”œâ”€â”€ data/                    # Almacenamiento temporal (gitignored)
â”œâ”€â”€ logs/                    # Logs de ejecuciÃ³n
â”œâ”€â”€ requirements.txt         # Dependencias
â””â”€â”€ watermarks.json          # Estado de extracciÃ³n incremental
```

---

## ğŸš€ EjecuciÃ³n Capa RAW

Para actualizar la capa Raw desde cero o incrementalmente:

```bash
# 1. Extraer datos nuevos de Supabase
python scripts/extract_from_supabase.py

# 2. Subir y particionar en GCS
python scripts/upload_to_gcs.py

# 3. Actualizar definiciones en BigQuery (si cambiÃ³ el schema)
python scripts/create_external_tables.py
```

---

## ğŸ›¡ï¸ Seguridad

*   **NUNCA** subir credenciales al repositorio.
*   Usar `.env` para secretos locales.
*   El archivo `watermarks.json` mantiene el estado de la extracciÃ³n, no borrar a menos que se desee un Full Load.

---

**Hecho con ğŸ’œ para Salvando Patitas**
