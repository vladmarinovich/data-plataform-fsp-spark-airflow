
# ‚ö° INICIO R√ÅPIDO - Operaci√≥n Diaria (Raw Layer)

Gu√≠a para ejecutar la carga de datos diaria (Capa Raw) de Salvando Patitas.

---

## üìã Pre-requisitos

1.  Estar en directorio del proyecto:
    ```bash
    cd pyspark-airflow-data-platform
    ```
2.  Activar entorno virtual:
    ```bash
    source venv/bin/activate
    ```
3.  Tener credenciales configuradas en `.env` (Supabase + GCP).

---

## üöÄ Ejecuci√≥n de Carga Diaria (RAW)

Para sincronizar Supabase con BigQuery (Data Lake):

### 1Ô∏è‚É£ Extraer Datos Nuevos (Incremental)
Descarga solo lo modificado desde la √∫ltima ejecuci√≥n (guarda estado en `watermarks.json`).

```bash
python scripts/extract_from_supabase.py
```

### 2Ô∏è‚É£ Subir a GCS y Particionar
Sube parquets a `gs://salvando-patitas-spark-raw/` organizados por `anio/mes/dia`. Corrige tipos de dato (IDs, Fechas).

```bash
python scripts/upload_to_gcs.py
```

### 3Ô∏è‚É£ Actualizar BigQuery External Tables
Asegura que BigQuery reconozca nuevas particiones y cambios de esquema.

```bash
python scripts/create_external_tables.py
```

---

## üîç Verificaci√≥n

Para confirmar que todo sali√≥ bien, ejecuta este query en BigQuery:

```sql
-- Verificar carga de hoy
SELECT table_name, count(*) as total_rows 
FROM `salvando-patitas-de-spark.raw.INFORMATION_SCHEMA.TABLES`
WHERE table_name LIKE 'raw_%'
GROUP BY table_name;
```

---

## üõ†Ô∏è Soluci√≥n de Problemas Frecuentes

*   **Error por falta de credenciales**: Revisa `export GOOGLE_APPLICATION_CREDENTIALS="..."`.
*   **Error de tipos en BigQuery**: Aseg√∫rate de haber ejecutado el paso 3 (`create_external_tables.py`) que recrea las definiciones.
*   **Quiero recargar todo desde cero**: Borra `watermarks.json` y ejecuta los pasos 1, 2 y 3.

