# üîß Soluci√≥n: Autenticaci√≥n GCS con Python SDK

**Fecha**: 29-Dic-2025  
**Problema**: Error 401 al subir archivos a GCS desde Docker  
**Soluci√≥n**: Migraci√≥n de `gsutil` a `google-cloud-storage` Python SDK

---

## üîç DIAGN√ìSTICO DEL PROBLEMA

### Problema Original
```
ServiceException: 401 Anonymous caller does not have storage.objects.create access
```

### Causa Ra√≠z
1. **Credenciales ADC (Application Default Credentials)**:
   - Tipo: Credenciales de **usuario** (no service account)
   - Archivo: `~/.config/gcloud/application_default_credentials.json`
   - Creadas con: `gcloud auth application-default login`

2. **Limitaci√≥n en Docker**:
   - `gsutil` requiere sesi√≥n activa de `gcloud auth`
   - ADC de usuario NO funciona autom√°ticamente con `gsutil` en contenedores
   - El archivo JSON est√° montado pero NO activado

3. **Pol√≠tica Organizacional**:
   - La organizaci√≥n bloquea creaci√≥n de service account keys
   - Error: "An Organization Policy that blocks service accounts key creation has been enforced"
   - No es posible crear service account tradicional

---

## ‚úÖ SOLUCI√ìN IMPLEMENTADA

### Migraci√≥n a Python SDK

**Archivo modificado**: `scripts/extract_from_supabase.py` (l√≠neas 268-291)

**Antes (gsutil)**:
```python
cmd = f"gsutil -m cp -r {table_path}/* {gcs_path}/"
result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
```

**Despu√©s (Python SDK)**:
```python
from google.cloud import storage

storage_client = storage.Client()  # Usa autom√°ticamente ADC
bucket = storage_client.bucket("salvando-patitas-spark")

for root, dirs, files in os.walk(table_path):
    for file in files:
        local_file = os.path.join(root, file)
        rel_path = os.path.relpath(local_file, table_path)
        blob_path = f"lake/raw/{table_name}/{rel_path}"
        
        blob = bucket.blob(blob_path)
        blob.upload_from_filename(local_file)
```

---

## üéØ VENTAJAS DE LA SOLUCI√ìN

1. ‚úÖ **Funciona con ADC**: No requiere service account keys
2. ‚úÖ **Cumple pol√≠ticas**: Compatible con restricciones organizacionales
3. ‚úÖ **M√°s robusto**: Mejor manejo de errores que subprocess
4. ‚úÖ **M√°s r√°pido**: No hay overhead de spawning procesos
5. ‚úÖ **Mejor logging**: Control granular de qu√© archivos se suben
6. ‚úÖ **Pythonic**: C√≥digo m√°s limpio y mantenible

---

## üìã CONFIGURACI√ìN ACTUAL

### Docker Compose
```yaml
volumes:
  - ~/.config/gcloud:/root/.config/gcloud:ro

environment:
  - GOOGLE_APPLICATION_CREDENTIALS=/root/.config/gcloud/application_default_credentials.json
  - ENV=cloud
```

### Requirements
```
google-cloud-storage>=2.13.0  # ‚úÖ Ya incluido
```

---

## üöÄ PR√ìXIMOS PASOS

### 1. Reconstruir Imagen Docker
```bash
cd "/Users/vladislavmarinovich/Library/CloudStorage/GoogleDrive-consultor@vladmarinovich.com/Shared drives/Vladislav/Salvando Patitas (SPDP) S-A/pyspark-airflow-data-platform"

docker-compose down
docker-compose build --no-cache
docker-compose up -d
```

### 2. Verificar Logs
```bash
# Ver logs del scheduler
docker-compose logs -f airflow-scheduler

# Ver estado de contenedores
docker-compose ps
```

### 3. Trigger DAG Manual
```bash
# Desde la UI: http://localhost:8080
# O desde CLI:
docker-compose exec -T airflow-scheduler airflow dags trigger spdp_data_platform_main
```

### 4. Validar Subida a GCS
```bash
# Verificar archivos en GCS
gsutil ls -r gs://salvando-patitas-spark/lake/raw/

# O desde Python
python3 scripts/verify_cloud_data.py
```

---

## üîê SEGURIDAD

### Credenciales ADC
- ‚úÖ **Archivo protegido**: En `.gitignore`
- ‚úÖ **Montado read-only**: `:ro` en docker-compose
- ‚úÖ **Scope limitado**: Solo permisos de tu cuenta GCP

### Alternativa Futura (Si se levanta la pol√≠tica)
Si en el futuro se permite crear service account keys:
1. Crear service account con roles:
   - Storage Object Admin
   - BigQuery Data Editor
   - BigQuery Job User
2. Descargar key JSON
3. Actualizar `GOOGLE_APPLICATION_CREDENTIALS` en docker-compose
4. El c√≥digo Python SDK funcionar√° igual (sin cambios)

---

## üìä TESTING

### Test Local (Antes de Docker)
```bash
# Activar venv
source venv/bin/activate

# Test de autenticaci√≥n
python3 -c "from google.cloud import storage; print(storage.Client().list_buckets())"

# Debe listar tus buckets sin error
```

### Test en Docker
```bash
# Entrar al contenedor
docker-compose exec airflow-scheduler bash

# Test de autenticaci√≥n
python3 -c "from google.cloud import storage; client = storage.Client(); print('‚úÖ Auth OK')"
```

---

## üí° NOTAS IMPORTANTES

1. **ADC Expiration**: Las credenciales ADC pueden expirar. Si ves errores de auth:
   ```bash
   gcloud auth application-default login
   ```

2. **Permisos**: Aseg√∫rate de tener los roles necesarios en GCP:
   - Storage Object Admin (en el bucket)
   - BigQuery Data Editor (en el dataset)

3. **Logs Detallados**: El nuevo c√≥digo imprime cada archivo subido:
   ```
   ‚úÖ Uploaded: lake/raw/donaciones/y=2023/m=01/file.parquet
   ```

---

## ‚úÖ CHECKLIST DE VALIDACI√ìN

Despu√©s de implementar:

- [ ] Docker Compose reconstruido
- [ ] Contenedores levantados sin errores
- [ ] DAG visible en Airflow UI
- [ ] Trigger manual ejecutado
- [ ] Logs muestran "‚úÖ Subida a GCS completada"
- [ ] Archivos visibles en GCS bucket
- [ ] No hay errores 401
- [ ] Watermark actualizado

---

**√öltima actualizaci√≥n**: 2025-12-29 09:35  
**Estado**: ‚úÖ C√≥digo modificado, listo para testing
